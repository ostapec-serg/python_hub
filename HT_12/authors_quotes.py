import requestsimport sqlite3from bs4 import BeautifulSoupimport csvdef quotes_to_scrap():    def table_check():        try:            with sqlite3.connect('data_base.db') as f:                cursor = f.cursor()                cursor.execute("DROP TABLE IF EXISTS author_quotes_about")                cursor.execute("""CREATE TABLE IF NOT EXISTS author_quotes_about                (id INTEGER, author TEXT, quotes TEXT, born_date TEXT,                 born_location TEXT, about TEXT, PRIMARY KEY("id" AUTOINCREMENT))""")                get_info()        except sqlite3.OperationalError:            get_info()    def get_info():        author_quote = []        about_links = set()        author_list = set()        url = 'http://quotes.toscrape.com'        while True:            try:                page_response = requests.get(url)                soup = BeautifulSoup(page_response.text, "lxml")                authors = soup.find_all('small', class_='author')                quote = soup.find_all('span', class_='text')                for elem in range(0, len(quote)):                    author_quote_dict = dict()                    author_quote_dict[authors[elem].text] = quote[elem].text                    author_quote.append(author_quote_dict)                    author_list.add(authors[elem].text)                about = soup.find_all('a', string='(about)')                for link in about:                    for _ in link:                        about_links.add((link.get('href')))                page = soup.find('li', class_='next').find('a').get('href')                url = 'http://quotes.toscrape.com'                url = url + page            except AttributeError:                write_info(author_quote, sorted(about_links), author_list)                break            except requests.exceptions.Timeout:                print("Server not responding, please try again later!")            except requests.exceptions.TooManyRedirects:                print("Possibly invalid URL")            except requests.ConnectionError:                print("No connection to server")    def write_info(author_quote, about_links, author_list):        try:            with open(f"about_author.csv", mode="w",                      encoding="utf-8", newline="") as file:                fieldnames = ['author', 'quotes', 'born_date',                              'born_location', 'author_description']                writer = csv.DictWriter(file, delimiter=",",                                        fieldnames=fieldnames)                writer.writeheader()                for link in about_links:                    url = 'http://quotes.toscrape.com'                    url = url + link                    page_response = requests.get(url)                    soup = BeautifulSoup(page_response.text, 'lxml')                    author_title = soup.find(class_='author-title').text.strip()                    born_date = soup.find(class_='author-born-date').text.strip()                    born_location = soup.find(class_='author-born-location').text.strip()                    description = soup.find(class_='author-description').text.strip()                    one_author_quotes = " "                    for data in author_list:                        for item in author_quote:                            for author, quotes in item.items():                                if data == author == author_title.replace('-', ' '):                                    one_author_quotes += quotes                    with sqlite3.connect('data_base.db') as db:                        cursor = db.cursor()                        cursor.execute("""INSERT INTO author_quotes_about(author,                                     quotes, born_date, born_location, about)                                     VALUES(?, ?, ?, ?, ?)""", (                                    author_title, one_author_quotes,                                    born_date, born_location, description)                                       )                    writer.writerow({'author': author_title, 'quotes':                                    one_author_quotes, 'born_date': born_date,                                    'born_location': born_location,                                     'author_description': description})            print("Information recorded")        except requests.exceptions.Timeout:            print("Server not responding, please try again later!")        except requests.exceptions.TooManyRedirects:            print("Possibly invalid URL")        except requests.ConnectionError:            print("No connection to server")        except sqlite3.OperationalError:            print("Write error!")    table_check()quotes_to_scrap()
import requestsimport sqlite3from bs4 import BeautifulSoupimport csvtry:    with sqlite3.connect('data_base.db') as f:        cur = f.cursor()        cur.execute("""CREATE TABLE IF NOT EXISTS author_quotes_about(id INTEGER, author TEXT,         quotes TEXT, born_date TEXT, born_location TEXT, about TEXT, PRIMARY KEY("id" AUTOINCREMENT)) """)except sqlite3.OperationalError:    passauthor_quote = []about_links = set()author_list = set()url = 'http://quotes.toscrape.com'try:    while True:        try:            page_response = requests.get(url)            soup = BeautifulSoup(page_response.text, "lxml")            authors = soup.find_all('small', class_='author')            quote = soup.find_all('span', class_='text')            for elem in range(0, len(quote)):                author_quote_dict = dict()                author_quote_dict[authors[elem].text] = quote[elem].text                author_quote.append(author_quote_dict)                author_list.add(authors[elem].text)            about = soup.find_all('a', string='(about)')            for link in about:                for element in link:                    about_links.add((link.get('href')))            next_page = soup.find('li', class_='next').find('a').get('href')            url = 'http://quotes.toscrape.com'            url = url+next_page        except AttributeError:            break    with sqlite3.connect('data_base.db') as db:        cursor = db.cursor()        with open(f"about_author.csv", mode="w", encoding="utf-8", newline="") as file:            fieldnames = ['author', 'quotes', 'born_date', 'born_location', 'author_description']            writer = csv.DictWriter(file, delimiter=",", fieldnames=fieldnames)            writer.writeheader()            for link in about_links:                url = 'http://quotes.toscrape.com'                url = url + link                page_response = requests.get(url)                soup = BeautifulSoup(page_response.text, 'lxml')                author_title = soup.find(class_='author-title').text.strip()                born_date = soup.find(class_='author-born-date').text.strip()                born_location = soup.find(class_='author-born-location').text.strip()                author_description = soup.find(class_='author-description').text.strip()                one_author_quotes = " "                for data in author_list:                    for item in author_quote:                        for author, quotes in item.items():                            if data == author == author_title.replace('-', ' '):                                one_author_quotes += quotes                cursor = db.cursor()                cursor.execute("""INSERT INTO author_quotes_about(author, quotes,                                 born_date, born_location, about)                                 VALUES(?, ?, ?, ?, ?)""",                               (author_title, one_author_quotes, born_date, born_location, author_description))                writer.writerow({'author': author_title, 'quotes': one_author_quotes, 'born_date': born_date,                                'born_location': born_location, 'author_description': author_description})except requests.exceptions.Timeout:    print("Сервер не отвечает, попробуйте позже!")except requests.exceptions.TooManyRedirects:    print("Вероятно некорректный URL")except requests.ConnectionError:    print("Нет соединения с сервером")except sqlite3.OperationalError:    print("Write error!")